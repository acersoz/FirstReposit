{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9171\n",
       " 0.9191\n",
       " 0.9250\n",
       " 0.9657\n",
       " 0.8656\n",
       " 0.8190\n",
       " 0.8340\n",
       " 0.9118\n",
       " 0.9263\n",
       " 0.8555\n",
       " 0.0555\n",
       " 0.0435\n",
       " 0.0708\n",
       " 0.0737\n",
       " 0.0429\n",
       " 0.0656\n",
       " 0.0744\n",
       " 0.0835\n",
       " 0.0399\n",
       " 0.0833\n",
       " 0.0080\n",
       " 0.0027\n",
       " 0.0284\n",
       " 0.0204\n",
       " 0.0118\n",
       " 0.0083\n",
       " 0.0045\n",
       " 0.0055\n",
       " 0.0119\n",
       " 0.0191\n",
       "[torch.FloatTensor of size 30x1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ahmet Can Ers√∂z\n",
    "#2016719051\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import scipy.stats  as stats\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('Iris.csv',header=0)\n",
    "\n",
    "#Since the methods naive bayes and logistic regression needs to be compared, the Iris data is divided into training and test data\n",
    "#sets. The indexes divisible with 5 are put into test set and the others into training set. To accomplish this, the indexes are\n",
    "#determined.\n",
    "all_index = np.arange(150)\n",
    "test_index = np.arange(0,150,5)\n",
    "train_index = np.setdiff1d(all_index,test_index)\n",
    "\n",
    "df_train = df.loc[train_index]\n",
    "df_test = df.loc[test_index]\n",
    "\n",
    "iris = df.values\n",
    "iris_train = df_train.values\n",
    "iris_test = df_test.values\n",
    "\n",
    "#The 4 features of the dataset are put into the X_train and X_test matrices\n",
    "X = np.random.randn(150,4)\n",
    "X = iris[:, [0, 1, 2, 3]]\n",
    "X_train = iris_train[:, [0, 1, 2, 3]]\n",
    "X_test = iris_test[:, [0, 1, 2, 3]]\n",
    "X = X.astype(np.float)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "#The classes of the dataset are put into the Y_train and Y_test matrices\n",
    "Y = iris[:, [4]]\n",
    "Y_train = iris_train[:, [4]]\n",
    "Y_test = iris_test[:, [4]]\n",
    "\n",
    "#The setosa classes are updated as 1 and the others 0\n",
    "Y[Y == 'Iris-setosa'] = 1\n",
    "Y[Y == 'Iris-versicolor'] = 0\n",
    "Y[Y == 'Iris-virginica'] = 0\n",
    "Y_train[Y_train == 'Iris-setosa'] = 1\n",
    "Y_train[Y_train == 'Iris-versicolor'] = 0\n",
    "Y_train[Y_train == 'Iris-virginica'] = 0\n",
    "Y_test[Y_test == 'Iris-setosa'] = 1\n",
    "Y_test[Y_test == 'Iris-versicolor'] = 0\n",
    "Y_test[Y_test == 'Iris-virginica'] = 0\n",
    "\n",
    "Y = Y.astype(np.float)\n",
    "Y_train = Y_train.astype(np.float)\n",
    "Y_test = Y_test.astype(np.float)\n",
    "\n",
    "X_train_var = Variable(torch.from_numpy(X_train))\n",
    "X_test_var  = Variable(torch.from_numpy(X_test))\n",
    "Y_train_var = Variable(torch.from_numpy(Y_train))\n",
    "Y_test_var  = Variable(torch.from_numpy(Y_test))\n",
    "\n",
    "#f = torch.nn.Linear(1, 1, bias=True)\n",
    "\n",
    "# Set w_1\n",
    "#f.weight.data = torch.FloatTensor([[1.]])\n",
    "# Set w_0\n",
    "#f.bias.data = torch.FloatTensor([[2.]])\n",
    "\n",
    "X_train_var = X_train_var.type(torch.FloatTensor)\n",
    "X_test_var  = X_test_var.type(torch.FloatTensor)\n",
    "Y_train_var = Y_train_var.type(torch.FloatTensor)\n",
    "Y_test_var  = Y_test_var.type(torch.FloatTensor)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 1, bias= False)  # One in and one out\n",
    "        #self.parameters.data = torch.FloatTensor([[0.],[0.],[1.],[1.]])  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.\n",
    "        \"\"\"\n",
    "        y_pred = torch.nn.functional.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "f = Model()\n",
    "\n",
    "# learning rate\n",
    "eta = 0.02\n",
    "\n",
    "Loss = torch.nn.BCELoss(size_average=True)\n",
    "#optimizer = torch.optim.SGD(f.parameters(), lr=0.01)\n",
    "w_tensor = torch.FloatTensor([[0., 0., 1., 1.]])\n",
    "#print('w', w_tensor)\n",
    "\n",
    "for param in f.parameters():\n",
    "    param.data = w_tensor\n",
    "\n",
    "#for param in f.parameters():\n",
    "#    print('initial', param.data)\n",
    "\n",
    "for epoch in range(500):\n",
    "    ## Compute the forward pass\n",
    "    E = Loss(f(X_train_var), Y_train_var)\n",
    "\n",
    "    #if epoch%1 == 0: \n",
    "        #print(epoch,':',E.data[0])\n",
    "#        print(f.bias.data.numpy())\n",
    "#        print(f.weight.data.numpy())\n",
    "\n",
    "    # Compute the gradients by automated differentiation\n",
    "    E.backward()\n",
    "    \n",
    "    # For each adjustable parameter \n",
    "    # Move along the negative gradient direction\n",
    "    for param in f.parameters():\n",
    "        param.data.add_(-eta * param.grad.data)\n",
    "        #print('param', param.data)\n",
    "        #print('param_grad', param.grad.data)\n",
    "\n",
    "    # Reset the gradients, as otherwise they are accumulated in param.grad\n",
    "    f.zero_grad()\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    #E.backward()\n",
    "    #optimizer.step()\n",
    "\n",
    "#print('Weights')\n",
    "#print(f.weight.data, f.bias.data)\n",
    "#for param in f.parameters():\n",
    "#    print('x')\n",
    "#    print(param.data)\n",
    "#    #print(param.weight.data)\n",
    "\n",
    "\n",
    "#print(f.weight.data.numpy())\n",
    "\n",
    "abc = f(X_test_var)\n",
    "\n",
    "abc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
